{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#GPU PROJECT"
      ],
      "metadata": {
        "id": "q7GU_90vNPHk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "CEPARCO-S11\n",
        "GROUP 4\n",
        "TOPIC: Hadamard product using 2D variables\n",
        "MEMBERS:\n",
        "    Arca, Althea Denisse\n",
        "    Co Chiong, Sean\n",
        "    Uy, Wesley King\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "fFjhGkKsKurl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WRITEFILE CODES\n",
        "\n",
        "000 - 1024x1024 - 8x8\n",
        "\n",
        "001 - 1024x1024 - 16x16\n",
        "\n",
        "010 - 1024x1024 - 32x32\n",
        "\n",
        "011 - 2048x2048 - 8x8\n",
        "\n",
        "100 - 2048x2048 - 16x16\n",
        "\n",
        "101 - 2048x2048 - 32x32\n",
        "\n",
        "110 - 4096x4096 - 8x8\n",
        "\n",
        "111 - 4096x4096 - 16x16\n",
        "\n",
        "1000 - 4096x4096 - 32x32\n",
        "\n"
      ],
      "metadata": {
        "id": "lX5aAtP0zTbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Dive: Hadamard product using 2D variables"
      ],
      "metadata": {
        "id": "g2bwJlDoM86S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1024x1024"
      ],
      "metadata": {
        "id": "xl3eZli2-0eZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 1024x1024 THREADSIZE: 8x8\n",
        "WRITE FILE = hadamard000"
      ],
      "metadata": {
        "id": "cuAEMIDvs4xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IocGa_s5shXv",
        "outputId": "0438750b-108e-46b9-fc80-d7fbbdafd2a6",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Wed Feb 19 15:24:39 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard000.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10  // Ensures values are between 0 and 10\n",
        "#define ARRAY_SIZE 1024 // Updated matrix size\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name);\n",
        "\n",
        "__global__\n",
        "void cuda_hadamard(size_t numCol, size_t numRow, float* Z, float* X, float* Y) {\n",
        "    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < numRow && col < numCol) {\n",
        "        size_t idx = row * numCol + col;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = ARRAY_SIZE * ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    // Unified Memory Allocation\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        for (size_t j = 0; j < ARRAY_SIZE; ++j) {\n",
        "            size_t idx = i * ARRAY_SIZE + j;\n",
        "            X[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "            Y[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Configure kernel launch parameters\n",
        "    dim3 threadsPerBlock(8, 8);\n",
        "    dim3 numBlocks((ARRAY_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ARRAY_SIZE + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Timing setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard<<<numBlocks, threadsPerBlock>>>(ARRAY_SIZE, ARRAY_SIZE, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before printing results\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Print initial matrices (only first 10x10 elements)\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, X, \"X\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Y, \"Y\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Z, \"Z\");\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// Corrected function to print only the first 10x10 elements\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name) {\n",
        "    printf(\"\\nMatrix %s (First 10x10 elements):\\n\", name);\n",
        "    for (size_t i = 0; i < 10; ++i) {  // Print first 10 rows\n",
        "        for (size_t j = 0; j < 10; ++j) {  // Print first 10 columns\n",
        "            printf(\"%.2f \", Arr[i * numCol + j]);  // Corrected indexing\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3tS6txQNd6a",
        "outputId": "0fb9f5bc-3a1d-4c42-cef8-bb0b8da2d8db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CUDA_hadamard000.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard000 CUDA_hadamard000.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehToGfjBaBVM",
        "outputId": "67ff3096-74ab-4c4f-adf9-7582673f51e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==358== NVPROF is profiling process 358, command: ./CUDA_hadamard000\n",
            "Average execution time over 30 runs: 0.0732 ms\n",
            "\n",
            "Matrix X (First 10x10 elements):\n",
            "7.00 9.00 1.00 6.00 6.00 3.00 3.00 8.00 1.00 9.00 \n",
            "10.00 1.00 5.00 6.00 4.00 0.00 2.00 5.00 2.00 5.00 \n",
            "4.00 5.00 0.00 5.00 5.00 6.00 4.00 8.00 8.00 4.00 \n",
            "3.00 5.00 7.00 4.00 7.00 2.00 9.00 0.00 2.00 10.00 \n",
            "9.00 3.00 4.00 6.00 3.00 9.00 4.00 3.00 5.00 1.00 \n",
            "4.00 0.00 10.00 9.00 3.00 4.00 1.00 0.00 1.00 1.00 \n",
            "7.00 8.00 8.00 8.00 5.00 9.00 0.00 8.00 0.00 1.00 \n",
            "2.00 6.00 9.00 0.00 7.00 6.00 9.00 10.00 9.00 2.00 \n",
            "8.00 7.00 0.00 0.00 6.00 10.00 8.00 6.00 7.00 9.00 \n",
            "2.00 6.00 1.00 1.00 10.00 7.00 6.00 8.00 5.00 7.00 \n",
            "\n",
            "Matrix Y (First 10x10 elements):\n",
            "3.00 10.00 1.00 2.00 10.00 6.00 0.00 7.00 5.00 7.00 \n",
            "8.00 0.00 5.00 9.00 4.00 5.00 6.00 0.00 10.00 9.00 \n",
            "6.00 9.00 3.00 6.00 8.00 2.00 7.00 8.00 7.00 0.00 \n",
            "9.00 3.00 3.00 9.00 10.00 10.00 5.00 0.00 6.00 8.00 \n",
            "3.00 1.00 10.00 10.00 2.00 5.00 7.00 4.00 8.00 8.00 \n",
            "0.00 7.00 5.00 2.00 0.00 6.00 9.00 2.00 8.00 10.00 \n",
            "6.00 0.00 8.00 0.00 4.00 8.00 8.00 10.00 8.00 0.00 \n",
            "1.00 7.00 8.00 5.00 8.00 0.00 1.00 1.00 10.00 5.00 \n",
            "7.00 0.00 10.00 0.00 10.00 9.00 6.00 7.00 3.00 1.00 \n",
            "6.00 3.00 5.00 9.00 4.00 4.00 10.00 1.00 7.00 2.00 \n",
            "\n",
            "Matrix Z (First 10x10 elements):\n",
            "21.00 90.00 1.00 12.00 60.00 18.00 0.00 56.00 5.00 63.00 \n",
            "80.00 0.00 25.00 54.00 16.00 0.00 12.00 0.00 20.00 45.00 \n",
            "24.00 45.00 0.00 30.00 40.00 12.00 28.00 64.00 56.00 0.00 \n",
            "27.00 15.00 21.00 36.00 70.00 20.00 45.00 0.00 12.00 80.00 \n",
            "27.00 3.00 40.00 60.00 6.00 45.00 28.00 12.00 40.00 8.00 \n",
            "0.00 0.00 50.00 18.00 0.00 24.00 9.00 0.00 8.00 10.00 \n",
            "42.00 0.00 64.00 0.00 20.00 72.00 0.00 80.00 0.00 0.00 \n",
            "2.00 42.00 72.00 0.00 56.00 0.00 9.00 10.00 90.00 10.00 \n",
            "56.00 0.00 0.00 0.00 60.00 90.00 48.00 42.00 21.00 9.00 \n",
            "12.00 18.00 5.00 9.00 40.00 28.00 60.00 8.00 35.00 14.00 \n",
            "==358== Profiling application: ./CUDA_hadamard000\n",
            "==358== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  1.7478ms        30  58.258us  56.863us  58.624us  cuda_hadamard(unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   97.13%  257.69ms         3  85.898ms  30.652us  257.60ms  cudaMallocManaged\n",
            "                    0.77%  2.0379ms         3  679.29us  156.81us  1.3138ms  cudaMemPrefetchAsync\n",
            "                    0.70%  1.8567ms        30  61.890us  53.667us  64.308us  cudaEventSynchronize\n",
            "                    0.41%  1.0817ms         5  216.35us  1.7820us  1.0673ms  cudaMemAdvise\n",
            "                    0.33%  864.10us         3  288.03us  260.61us  305.40us  cudaFree\n",
            "                    0.32%  861.18us         1  861.18us  861.18us  861.18us  cuDeviceGetPCIBusId\n",
            "                    0.15%  406.04us        30  13.534us  3.7230us  194.42us  cudaLaunchKernel\n",
            "                    0.06%  171.51us       114  1.5040us     106ns  59.223us  cuDeviceGetAttribute\n",
            "                    0.06%  168.09us        60  2.8010us  1.2200us  14.052us  cudaEventRecord\n",
            "                    0.03%  84.501us         2  42.250us  4.7760us  79.725us  cudaDeviceSynchronize\n",
            "                    0.02%  45.024us        30  1.5000us     928ns  2.8060us  cudaEventElapsedTime\n",
            "                    0.00%  12.314us         2  6.1570us     994ns  11.320us  cudaEventCreate\n",
            "                    0.00%  10.887us         1  10.887us  10.887us  10.887us  cuDeviceGetName\n",
            "                    0.00%  5.0410us        30     168ns      97ns     507ns  cudaGetLastError\n",
            "                    0.00%  4.0640us         2  2.0320us     800ns  3.2640us  cudaEventDestroy\n",
            "                    0.00%  2.6990us         2  1.3490us     168ns  2.5310us  cuDeviceGet\n",
            "                    0.00%  2.3910us         1  2.3910us  2.3910us  2.3910us  cudaGetDevice\n",
            "                    0.00%  2.1950us         3     731ns     131ns  1.5280us  cuDeviceGetCount\n",
            "                    0.00%     822ns         1     822ns     822ns     822ns  cuDeviceTotalMem\n",
            "                    0.00%     486ns         1     486ns     486ns     486ns  cuModuleGetLoadingMode\n",
            "                    0.00%     379ns         1     379ns     379ns     379ns  cuDeviceGetUuid\n",
            "\n",
            "==358== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "       4  2.0000MB  2.0000MB  2.0000MB  8.000000MB  694.9330us  Host To Device\n",
            "       2  32.000KB  4.0000KB  60.000KB  64.00000KB  9.184000us  Device To Host\n",
            "Total CPU Page faults: 2049\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 1024x1024 THREADSIZE: 16x16\n",
        "WRITE FILE = hadamard001"
      ],
      "metadata": {
        "id": "uhn8uSGT7F0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard001.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10  // Ensures values are between 0 and 10\n",
        "#define ARRAY_SIZE 1024 // Updated matrix size\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name);\n",
        "\n",
        "__global__\n",
        "void cuda_hadamard(size_t numCol, size_t numRow, float* Z, float* X, float* Y) {\n",
        "    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < numRow && col < numCol) {\n",
        "        size_t idx = row * numCol + col;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = ARRAY_SIZE * ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    // Unified Memory Allocation\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        for (size_t j = 0; j < ARRAY_SIZE; ++j) {\n",
        "            size_t idx = i * ARRAY_SIZE + j;\n",
        "            X[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "            Y[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Configure kernel launch parameters\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 numBlocks((ARRAY_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ARRAY_SIZE + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Timing setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard<<<numBlocks, threadsPerBlock>>>(ARRAY_SIZE, ARRAY_SIZE, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before printing results\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Print initial matrices (only first 10x10 elements)\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, X, \"X\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Y, \"Y\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Z, \"Z\");\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// Corrected function to print only the first 10x10 elements\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name) {\n",
        "    printf(\"\\nMatrix %s (First 10x10 elements):\\n\", name);\n",
        "    for (size_t i = 0; i < 10; ++i) {  // Print first 10 rows\n",
        "        for (size_t j = 0; j < 10; ++j) {  // Print first 10 columns\n",
        "            printf(\"%.2f \", Arr[i * numCol + j]);  // Corrected indexing\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce2Bfqks7NF_",
        "outputId": "7de1c568-0fb1-4093-a41f-5b37d4314cfe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CUDA_hadamard001.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard001 CUDA_hadamard001.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD_cHfKK9UNi",
        "outputId": "71222f96-299d-4fe3-d71d-bffdf15c5116"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==885== NVPROF is profiling process 885, command: ./CUDA_hadamard001\n",
            "Average execution time over 30 runs: 0.0738 ms\n",
            "\n",
            "Matrix X (First 10x10 elements):\n",
            "1.00 6.00 7.00 0.00 10.00 9.00 5.00 5.00 5.00 1.00 \n",
            "7.00 10.00 6.00 4.00 7.00 10.00 10.00 4.00 1.00 4.00 \n",
            "3.00 5.00 5.00 6.00 8.00 0.00 7.00 4.00 3.00 4.00 \n",
            "10.00 9.00 0.00 10.00 1.00 5.00 0.00 6.00 5.00 8.00 \n",
            "1.00 8.00 8.00 6.00 9.00 7.00 6.00 7.00 5.00 10.00 \n",
            "5.00 2.00 5.00 3.00 7.00 8.00 0.00 2.00 7.00 1.00 \n",
            "9.00 3.00 5.00 10.00 9.00 8.00 1.00 8.00 5.00 9.00 \n",
            "6.00 1.00 3.00 3.00 6.00 8.00 1.00 10.00 5.00 3.00 \n",
            "6.00 2.00 7.00 3.00 4.00 0.00 0.00 8.00 0.00 6.00 \n",
            "10.00 8.00 5.00 1.00 8.00 10.00 7.00 0.00 7.00 8.00 \n",
            "\n",
            "Matrix Y (First 10x10 elements):\n",
            "1.00 1.00 1.00 7.00 9.00 6.00 10.00 4.00 0.00 9.00 \n",
            "5.00 7.00 9.00 7.00 6.00 10.00 5.00 8.00 6.00 0.00 \n",
            "10.00 5.00 2.00 3.00 1.00 3.00 5.00 0.00 10.00 6.00 \n",
            "2.00 0.00 2.00 6.00 5.00 5.00 1.00 5.00 1.00 3.00 \n",
            "9.00 8.00 4.00 9.00 6.00 2.00 7.00 5.00 8.00 10.00 \n",
            "4.00 6.00 1.00 7.00 1.00 2.00 9.00 2.00 7.00 4.00 \n",
            "4.00 5.00 7.00 7.00 9.00 0.00 4.00 9.00 10.00 7.00 \n",
            "4.00 3.00 10.00 7.00 8.00 6.00 0.00 0.00 7.00 2.00 \n",
            "6.00 2.00 1.00 4.00 1.00 10.00 9.00 6.00 8.00 3.00 \n",
            "8.00 9.00 9.00 3.00 6.00 0.00 8.00 9.00 1.00 9.00 \n",
            "\n",
            "Matrix Z (First 10x10 elements):\n",
            "1.00 6.00 7.00 0.00 90.00 54.00 50.00 20.00 0.00 9.00 \n",
            "35.00 70.00 54.00 28.00 42.00 100.00 50.00 32.00 6.00 0.00 \n",
            "30.00 25.00 10.00 18.00 8.00 0.00 35.00 0.00 30.00 24.00 \n",
            "20.00 0.00 0.00 60.00 5.00 25.00 0.00 30.00 5.00 24.00 \n",
            "9.00 64.00 32.00 54.00 54.00 14.00 42.00 35.00 40.00 100.00 \n",
            "20.00 12.00 5.00 21.00 7.00 16.00 0.00 4.00 49.00 4.00 \n",
            "36.00 15.00 35.00 70.00 81.00 0.00 4.00 72.00 50.00 63.00 \n",
            "24.00 3.00 30.00 21.00 48.00 48.00 0.00 0.00 35.00 6.00 \n",
            "36.00 4.00 7.00 12.00 4.00 0.00 0.00 48.00 0.00 18.00 \n",
            "80.00 72.00 45.00 3.00 48.00 0.00 56.00 0.00 7.00 72.00 \n",
            "==885== Profiling application: ./CUDA_hadamard001\n",
            "==885== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  1.7770ms        30  59.232us  57.567us  59.679us  cuda_hadamard(unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   93.34%  245.03ms         3  81.677ms  20.710us  244.95ms  cudaMallocManaged\n",
            "                    4.90%  12.854ms         2  6.4269ms  5.5610us  12.848ms  cudaDeviceSynchronize\n",
            "                    0.72%  1.8852ms        30  62.841us  55.266us  69.050us  cudaEventSynchronize\n",
            "                    0.36%  957.70us         3  319.23us  14.130us  792.03us  cudaMemPrefetchAsync\n",
            "                    0.33%  859.73us         3  286.58us  211.23us  349.72us  cudaFree\n",
            "                    0.15%  400.02us        30  13.333us  5.6100us  201.31us  cudaLaunchKernel\n",
            "                    0.08%  199.61us       114  1.7500us     136ns  81.399us  cuDeviceGetAttribute\n",
            "                    0.06%  162.96us        60  2.7150us  1.9740us  12.815us  cudaEventRecord\n",
            "                    0.02%  52.166us        30  1.7380us  1.5800us  2.8380us  cudaEventElapsedTime\n",
            "                    0.01%  38.210us         1  38.210us  38.210us  38.210us  cuDeviceGetName\n",
            "                    0.01%  28.979us         5  5.7950us  1.8720us  18.702us  cudaMemAdvise\n",
            "                    0.01%  15.371us         2  7.6850us  1.0560us  14.315us  cudaEventCreate\n",
            "                    0.00%  7.8950us         1  7.8950us  7.8950us  7.8950us  cuDeviceGetPCIBusId\n",
            "                    0.00%  6.1870us        30     206ns     141ns     592ns  cudaGetLastError\n",
            "                    0.00%  3.8080us         2  1.9040us     710ns  3.0980us  cudaEventDestroy\n",
            "                    0.00%  2.4990us         1  2.4990us  2.4990us  2.4990us  cudaGetDevice\n",
            "                    0.00%  2.4910us         3     830ns     197ns  2.0330us  cuDeviceGetCount\n",
            "                    0.00%  1.2500us         2     625ns     258ns     992ns  cuDeviceGet\n",
            "                    0.00%     824ns         1     824ns     824ns     824ns  cuModuleGetLoadingMode\n",
            "                    0.00%     643ns         1     643ns     643ns     643ns  cuDeviceTotalMem\n",
            "                    0.00%     326ns         1     326ns     326ns     326ns  cuDeviceGetUuid\n",
            "\n",
            "==885== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "       4  2.0000MB  2.0000MB  2.0000MB  8.000000MB  695.3200us  Host To Device\n",
            "       2  32.000KB  4.0000KB  60.000KB  64.00000KB  9.152000us  Device To Host\n",
            "Total CPU Page faults: 2049\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 1024x1024 THREADSIZE: 32x32\n",
        "WRITE FILE = hadamard010"
      ],
      "metadata": {
        "id": "gtHJwfxP9cqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard010.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10  // Ensures values are between 0 and 10\n",
        "#define ARRAY_SIZE 1024 // Updated matrix size\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name);\n",
        "\n",
        "__global__\n",
        "void cuda_hadamard(size_t numCol, size_t numRow, float* Z, float* X, float* Y) {\n",
        "    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < numRow && col < numCol) {\n",
        "        size_t idx = row * numCol + col;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = ARRAY_SIZE * ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    // Unified Memory Allocation\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        for (size_t j = 0; j < ARRAY_SIZE; ++j) {\n",
        "            size_t idx = i * ARRAY_SIZE + j;\n",
        "            X[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "            Y[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Configure kernel launch parameters\n",
        "    dim3 threadsPerBlock(32, 32);\n",
        "    dim3 numBlocks((ARRAY_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ARRAY_SIZE + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Timing setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard<<<numBlocks, threadsPerBlock>>>(ARRAY_SIZE, ARRAY_SIZE, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before printing results\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Print initial matrices (only first 10x10 elements)\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, X, \"X\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Y, \"Y\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Z, \"Z\");\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// Corrected function to print only the first 10x10 elements\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name) {\n",
        "    printf(\"\\nMatrix %s (First 10x10 elements):\\n\", name);\n",
        "    for (size_t i = 0; i < 10; ++i) {  // Print first 10 rows\n",
        "        for (size_t j = 0; j < 10; ++j) {  // Print first 10 columns\n",
        "            printf(\"%.2f \", Arr[i * numCol + j]);  // Corrected indexing\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78C3gcxA9ojJ",
        "outputId": "43f6bd83-9e80-403f-d6e9-e915434b4505"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CUDA_hadamard010.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard010 CUDA_hadamard010.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard010"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bYskFfM9t6x",
        "outputId": "5a6f113b-9c5e-4cd3-8aed-a37d64e1429f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2250== NVPROF is profiling process 2250, command: ./CUDA_hadamard010\n",
            "Average execution time over 30 runs: 0.0741 ms\n",
            "\n",
            "Matrix X (First 10x10 elements):\n",
            "10.00 5.00 9.00 2.00 3.00 9.00 8.00 10.00 5.00 10.00 \n",
            "5.00 6.00 3.00 5.00 3.00 3.00 6.00 1.00 4.00 6.00 \n",
            "5.00 9.00 2.00 10.00 0.00 9.00 5.00 2.00 7.00 5.00 \n",
            "9.00 8.00 4.00 3.00 3.00 0.00 6.00 5.00 4.00 3.00 \n",
            "6.00 1.00 8.00 0.00 4.00 1.00 9.00 7.00 10.00 7.00 \n",
            "7.00 9.00 5.00 5.00 4.00 3.00 10.00 5.00 10.00 1.00 \n",
            "7.00 10.00 1.00 1.00 9.00 9.00 9.00 7.00 0.00 9.00 \n",
            "2.00 1.00 3.00 3.00 10.00 7.00 3.00 6.00 0.00 7.00 \n",
            "8.00 8.00 7.00 0.00 5.00 0.00 8.00 8.00 2.00 1.00 \n",
            "0.00 10.00 4.00 8.00 2.00 10.00 8.00 5.00 8.00 8.00 \n",
            "\n",
            "Matrix Y (First 10x10 elements):\n",
            "10.00 0.00 10.00 0.00 2.00 0.00 0.00 6.00 6.00 9.00 \n",
            "3.00 3.00 7.00 9.00 2.00 9.00 5.00 2.00 1.00 9.00 \n",
            "0.00 3.00 5.00 3.00 7.00 10.00 7.00 7.00 1.00 7.00 \n",
            "5.00 4.00 3.00 10.00 4.00 1.00 5.00 8.00 3.00 2.00 \n",
            "9.00 5.00 3.00 7.00 1.00 10.00 2.00 8.00 10.00 9.00 \n",
            "7.00 5.00 0.00 10.00 4.00 9.00 5.00 9.00 2.00 6.00 \n",
            "7.00 8.00 10.00 2.00 9.00 10.00 7.00 2.00 10.00 5.00 \n",
            "10.00 0.00 8.00 3.00 0.00 0.00 5.00 2.00 0.00 2.00 \n",
            "7.00 3.00 4.00 0.00 8.00 8.00 7.00 5.00 7.00 9.00 \n",
            "0.00 10.00 6.00 7.00 5.00 7.00 7.00 0.00 5.00 1.00 \n",
            "\n",
            "Matrix Z (First 10x10 elements):\n",
            "100.00 0.00 90.00 0.00 6.00 0.00 0.00 60.00 30.00 90.00 \n",
            "15.00 18.00 21.00 45.00 6.00 27.00 30.00 2.00 4.00 54.00 \n",
            "0.00 27.00 10.00 30.00 0.00 90.00 35.00 14.00 7.00 35.00 \n",
            "45.00 32.00 12.00 30.00 12.00 0.00 30.00 40.00 12.00 6.00 \n",
            "54.00 5.00 24.00 0.00 4.00 10.00 18.00 56.00 100.00 63.00 \n",
            "49.00 45.00 0.00 50.00 16.00 27.00 50.00 45.00 20.00 6.00 \n",
            "49.00 80.00 10.00 2.00 81.00 90.00 63.00 14.00 0.00 45.00 \n",
            "20.00 0.00 24.00 9.00 0.00 0.00 15.00 12.00 0.00 14.00 \n",
            "56.00 24.00 28.00 0.00 40.00 0.00 56.00 40.00 14.00 9.00 \n",
            "0.00 100.00 24.00 56.00 10.00 70.00 56.00 0.00 40.00 8.00 \n",
            "==2250== Profiling application: ./CUDA_hadamard010\n",
            "==2250== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  1.9080ms        30  63.599us  62.431us  64.575us  cuda_hadamard(unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   97.71%  215.51ms         3  71.836ms  25.861us  215.44ms  cudaMallocManaged\n",
            "                    0.93%  2.0467ms        30  68.223us  60.905us  70.725us  cudaEventSynchronize\n",
            "                    0.47%  1.0311ms         2  515.55us  5.6680us  1.0254ms  cudaDeviceSynchronize\n",
            "                    0.30%  670.93us         3  223.64us  14.024us  470.61us  cudaMemPrefetchAsync\n",
            "                    0.29%  650.19us         3  216.73us  179.83us  251.46us  cudaFree\n",
            "                    0.13%  284.14us        30  9.4710us  3.4770us  154.31us  cudaLaunchKernel\n",
            "                    0.07%  151.68us       114  1.3300us     107ns  67.496us  cuDeviceGetAttribute\n",
            "                    0.05%  112.03us        60  1.8670us  1.2390us  10.530us  cudaEventRecord\n",
            "                    0.02%  33.839us        30  1.1270us     973ns  2.2980us  cudaEventElapsedTime\n",
            "                    0.01%  22.591us         5  4.5180us  1.1180us  15.954us  cudaMemAdvise\n",
            "                    0.01%  18.663us         2  9.3310us  1.0740us  17.589us  cudaEventCreate\n",
            "                    0.01%  12.380us         1  12.380us  12.380us  12.380us  cuDeviceGetName\n",
            "                    0.00%  5.5740us         1  5.5740us  5.5740us  5.5740us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.4790us        30     115ns      92ns     505ns  cudaGetLastError\n",
            "                    0.00%  2.9550us         2  1.4770us     463ns  2.4920us  cudaEventDestroy\n",
            "                    0.00%  2.0330us         1  2.0330us  2.0330us  2.0330us  cudaGetDevice\n",
            "                    0.00%  1.5190us         3     506ns     129ns  1.0730us  cuDeviceGetCount\n",
            "                    0.00%     781ns         2     390ns     132ns     649ns  cuDeviceGet\n",
            "                    0.00%     712ns         1     712ns     712ns     712ns  cuModuleGetLoadingMode\n",
            "                    0.00%     355ns         1     355ns     355ns     355ns  cuDeviceTotalMem\n",
            "                    0.00%     281ns         1     281ns     281ns     281ns  cuDeviceGetUuid\n",
            "\n",
            "==2250== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "       4  2.0000MB  2.0000MB  2.0000MB  8.000000MB  693.3650us  Host To Device\n",
            "       2  32.000KB  4.0000KB  60.000KB  64.00000KB  9.408000us  Device To Host\n",
            "Total CPU Page faults: 2049\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dUVk9RfX-vKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2048x2048"
      ],
      "metadata": {
        "id": "v_93cNgKAxno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 2048x2048 THREADSIZE: 8x8\n",
        "WRITE FILE = hadamard011"
      ],
      "metadata": {
        "id": "6zAtF4UyA1hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard011.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10  // Ensures values are between 0 and 10\n",
        "#define ARRAY_SIZE 2048 // Updated matrix size\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name);\n",
        "\n",
        "__global__\n",
        "void cuda_hadamard(size_t numCol, size_t numRow, float* Z, float* X, float* Y) {\n",
        "    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < numRow && col < numCol) {\n",
        "        size_t idx = row * numCol + col;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = ARRAY_SIZE * ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    // Unified Memory Allocation\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        for (size_t j = 0; j < ARRAY_SIZE; ++j) {\n",
        "            size_t idx = i * ARRAY_SIZE + j;\n",
        "            X[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "            Y[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Configure kernel launch parameters\n",
        "    dim3 threadsPerBlock(8, 8);\n",
        "    dim3 numBlocks((ARRAY_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ARRAY_SIZE + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Timing setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard<<<numBlocks, threadsPerBlock>>>(ARRAY_SIZE, ARRAY_SIZE, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before printing results\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Print initial matrices (only first 10x10 elements)\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, X, \"X\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Y, \"Y\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Z, \"Z\");\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// Corrected function to print only the first 10x10 elements\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name) {\n",
        "    printf(\"\\nMatrix %s (First 10x10 elements):\\n\", name);\n",
        "    for (size_t i = 0; i < 10; ++i) {  // Print first 10 rows\n",
        "        for (size_t j = 0; j < 10; ++j) {  // Print first 10 columns\n",
        "            printf(\"%.2f \", Arr[i * numCol + j]);  // Corrected indexing\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j24FpvNfA85r",
        "outputId": "7079c629-aadd-47d0-f2c4-155a79c13362"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CUDA_hadamard011.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard011 CUDA_hadamard011.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard011"
      ],
      "metadata": {
        "id": "jM-y4NPesnJl",
        "outputId": "e85e2b1e-ec5e-41f0-ddcd-837fae3acf7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2587== NVPROF is profiling process 2587, command: ./CUDA_hadamard011\n",
            "Average execution time over 30 runs: 0.2252 ms\n",
            "\n",
            "Matrix X (First 10x10 elements):\n",
            "6.00 3.00 5.00 1.00 0.00 4.00 1.00 2.00 4.00 6.00 \n",
            "5.00 5.00 9.00 5.00 8.00 9.00 1.00 10.00 8.00 6.00 \n",
            "6.00 2.00 9.00 4.00 9.00 1.00 7.00 6.00 6.00 1.00 \n",
            "1.00 3.00 5.00 0.00 6.00 8.00 0.00 6.00 2.00 3.00 \n",
            "4.00 4.00 8.00 9.00 4.00 10.00 9.00 1.00 4.00 7.00 \n",
            "2.00 4.00 4.00 4.00 10.00 2.00 9.00 0.00 3.00 2.00 \n",
            "1.00 4.00 10.00 8.00 10.00 9.00 2.00 3.00 3.00 0.00 \n",
            "5.00 10.00 3.00 5.00 0.00 8.00 2.00 5.00 7.00 1.00 \n",
            "10.00 3.00 5.00 1.00 7.00 7.00 9.00 10.00 8.00 4.00 \n",
            "10.00 4.00 4.00 6.00 7.00 2.00 8.00 8.00 4.00 2.00 \n",
            "\n",
            "Matrix Y (First 10x10 elements):\n",
            "1.00 0.00 2.00 2.00 7.00 5.00 9.00 0.00 9.00 9.00 \n",
            "8.00 2.00 10.00 2.00 0.00 1.00 7.00 9.00 0.00 9.00 \n",
            "0.00 4.00 1.00 2.00 8.00 5.00 1.00 1.00 1.00 9.00 \n",
            "5.00 9.00 0.00 0.00 1.00 9.00 8.00 9.00 5.00 9.00 \n",
            "5.00 8.00 8.00 9.00 9.00 1.00 2.00 7.00 3.00 4.00 \n",
            "3.00 9.00 10.00 6.00 5.00 3.00 5.00 4.00 6.00 3.00 \n",
            "0.00 10.00 10.00 8.00 2.00 4.00 4.00 3.00 8.00 5.00 \n",
            "5.00 7.00 4.00 3.00 8.00 4.00 5.00 1.00 10.00 10.00 \n",
            "4.00 4.00 5.00 7.00 5.00 1.00 8.00 2.00 8.00 9.00 \n",
            "6.00 0.00 0.00 2.00 6.00 3.00 0.00 0.00 1.00 2.00 \n",
            "\n",
            "Matrix Z (First 10x10 elements):\n",
            "6.00 0.00 10.00 2.00 0.00 20.00 9.00 0.00 36.00 54.00 \n",
            "40.00 10.00 90.00 10.00 0.00 9.00 7.00 90.00 0.00 54.00 \n",
            "0.00 8.00 9.00 8.00 72.00 5.00 7.00 6.00 6.00 9.00 \n",
            "5.00 27.00 0.00 0.00 6.00 72.00 0.00 54.00 10.00 27.00 \n",
            "20.00 32.00 64.00 81.00 36.00 10.00 18.00 7.00 12.00 28.00 \n",
            "6.00 36.00 40.00 24.00 50.00 6.00 45.00 0.00 18.00 6.00 \n",
            "0.00 40.00 100.00 64.00 20.00 36.00 8.00 9.00 24.00 0.00 \n",
            "25.00 70.00 12.00 15.00 0.00 32.00 10.00 5.00 70.00 10.00 \n",
            "40.00 12.00 25.00 7.00 35.00 7.00 72.00 20.00 64.00 36.00 \n",
            "60.00 0.00 0.00 12.00 42.00 6.00 0.00 0.00 4.00 4.00 \n",
            "==2587== Profiling application: ./CUDA_hadamard011\n",
            "==2587== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  6.4075ms        30  213.58us  212.45us  214.46us  cuda_hadamard(unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   94.20%  214.00ms         3  71.332ms  40.930us  213.91ms  cudaMallocManaged\n",
            "                    2.87%  6.5224ms        30  217.41us  187.54us  219.83us  cudaEventSynchronize\n",
            "                    1.60%  3.6331ms         3  1.2110ms  174.47us  1.9963ms  cudaMemPrefetchAsync\n",
            "                    0.94%  2.1318ms         3  710.60us  687.61us  746.29us  cudaFree\n",
            "                    0.14%  314.40us        30  10.480us  3.5570us  175.88us  cudaLaunchKernel\n",
            "                    0.07%  163.12us         2  81.561us  6.6520us  156.47us  cudaDeviceSynchronize\n",
            "                    0.07%  152.00us       114  1.3330us     102ns  58.484us  cuDeviceGetAttribute\n",
            "                    0.05%  121.89us        60  2.0310us  1.2710us  14.130us  cudaEventRecord\n",
            "                    0.01%  32.481us         1  32.481us  32.481us  32.481us  cuDeviceGetName\n",
            "                    0.01%  32.405us        30  1.0800us     891ns  2.1070us  cudaEventElapsedTime\n",
            "                    0.01%  25.822us         2  12.911us  12.892us  12.930us  cudaEventCreate\n",
            "                    0.01%  19.592us         5  3.9180us  1.1810us  12.787us  cudaMemAdvise\n",
            "                    0.00%  4.7560us         1  4.7560us  4.7560us  4.7560us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.3100us        30     143ns     109ns     524ns  cudaGetLastError\n",
            "                    0.00%  3.2430us         2  1.6210us     525ns  2.7180us  cudaEventDestroy\n",
            "                    0.00%  2.3820us         1  2.3820us  2.3820us  2.3820us  cudaGetDevice\n",
            "                    0.00%  1.1940us         3     398ns     133ns     830ns  cuDeviceGetCount\n",
            "                    0.00%     583ns         2     291ns     119ns     464ns  cuDeviceGet\n",
            "                    0.00%     545ns         1     545ns     545ns     545ns  cuDeviceTotalMem\n",
            "                    0.00%     530ns         1     530ns     530ns     530ns  cuModuleGetLoadingMode\n",
            "                    0.00%     287ns         1     287ns     287ns     287ns  cuDeviceGetUuid\n",
            "\n",
            "==2587== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      16  2.0000MB  2.0000MB  2.0000MB  32.00000MB  2.774138ms  Host To Device\n",
            "       4  32.000KB  4.0000KB  60.000KB  128.0000KB  18.07900us  Device To Host\n",
            "Total CPU Page faults: 8194\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 2048x2048 THREADSIZE: 16x16\n",
        "WRITE FILE = hadamard100"
      ],
      "metadata": {
        "id": "_E-jEHZkCrzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard100.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10  // Ensures values are between 0 and 10\n",
        "#define ARRAY_SIZE 2048 // Updated matrix size\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name);\n",
        "\n",
        "__global__\n",
        "void cuda_hadamard(size_t numCol, size_t numRow, float* Z, float* X, float* Y) {\n",
        "    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < numRow && col < numCol) {\n",
        "        size_t idx = row * numCol + col;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = ARRAY_SIZE * ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    // Unified Memory Allocation\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        for (size_t j = 0; j < ARRAY_SIZE; ++j) {\n",
        "            size_t idx = i * ARRAY_SIZE + j;\n",
        "            X[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "            Y[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Configure kernel launch parameters\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 numBlocks((ARRAY_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ARRAY_SIZE + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Timing setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard<<<numBlocks, threadsPerBlock>>>(ARRAY_SIZE, ARRAY_SIZE, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before printing results\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Print initial matrices (only first 10x10 elements)\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, X, \"X\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Y, \"Y\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Z, \"Z\");\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// Corrected function to print only the first 10x10 elements\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name) {\n",
        "    printf(\"\\nMatrix %s (First 10x10 elements):\\n\", name);\n",
        "    for (size_t i = 0; i < 10; ++i) {  // Print first 10 rows\n",
        "        for (size_t j = 0; j < 10; ++j) {  // Print first 10 columns\n",
        "            printf(\"%.2f \", Arr[i * numCol + j]);  // Corrected indexing\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHUP3J9ktqCH",
        "outputId": "6aa84f17-8ade-4738-bdd4-e4a2ecb92188"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CUDA_hadamard100.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard100 CUDA_hadamard100.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJnc6cHdtvhK",
        "outputId": "9fcf1674-8f3e-4706-d055-2593a605a288"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==2850== NVPROF is profiling process 2850, command: ./CUDA_hadamard100\n",
            "Average execution time over 30 runs: 0.2257 ms\n",
            "\n",
            "Matrix X (First 10x10 elements):\n",
            "3.00 1.00 9.00 8.00 5.00 5.00 1.00 5.00 6.00 7.00 \n",
            "7.00 4.00 2.00 2.00 7.00 5.00 7.00 9.00 4.00 10.00 \n",
            "6.00 2.00 2.00 4.00 7.00 5.00 8.00 2.00 5.00 1.00 \n",
            "5.00 7.00 7.00 6.00 3.00 4.00 2.00 5.00 1.00 7.00 \n",
            "5.00 3.00 6.00 1.00 6.00 2.00 9.00 1.00 8.00 5.00 \n",
            "4.00 4.00 9.00 0.00 7.00 5.00 1.00 10.00 5.00 0.00 \n",
            "10.00 0.00 2.00 0.00 0.00 4.00 3.00 7.00 10.00 5.00 \n",
            "10.00 9.00 5.00 5.00 10.00 10.00 2.00 2.00 3.00 6.00 \n",
            "7.00 2.00 7.00 3.00 2.00 8.00 5.00 0.00 6.00 2.00 \n",
            "8.00 3.00 5.00 1.00 1.00 1.00 9.00 8.00 0.00 1.00 \n",
            "\n",
            "Matrix Y (First 10x10 elements):\n",
            "2.00 9.00 2.00 8.00 3.00 2.00 8.00 0.00 0.00 6.00 \n",
            "6.00 9.00 8.00 4.00 3.00 6.00 9.00 10.00 5.00 7.00 \n",
            "6.00 3.00 7.00 1.00 2.00 10.00 3.00 0.00 9.00 2.00 \n",
            "3.00 1.00 0.00 5.00 9.00 3.00 1.00 9.00 6.00 2.00 \n",
            "5.00 10.00 6.00 4.00 9.00 2.00 5.00 3.00 6.00 2.00 \n",
            "2.00 3.00 8.00 8.00 6.00 10.00 9.00 6.00 7.00 0.00 \n",
            "1.00 8.00 0.00 2.00 6.00 4.00 10.00 7.00 0.00 8.00 \n",
            "4.00 2.00 10.00 7.00 9.00 10.00 6.00 8.00 5.00 0.00 \n",
            "3.00 9.00 4.00 3.00 4.00 1.00 3.00 3.00 6.00 2.00 \n",
            "3.00 1.00 6.00 6.00 2.00 4.00 10.00 4.00 9.00 4.00 \n",
            "\n",
            "Matrix Z (First 10x10 elements):\n",
            "6.00 9.00 18.00 64.00 15.00 10.00 8.00 0.00 0.00 42.00 \n",
            "42.00 36.00 16.00 8.00 21.00 30.00 63.00 90.00 20.00 70.00 \n",
            "36.00 6.00 14.00 4.00 14.00 50.00 24.00 0.00 45.00 2.00 \n",
            "15.00 7.00 0.00 30.00 27.00 12.00 2.00 45.00 6.00 14.00 \n",
            "25.00 30.00 36.00 4.00 54.00 4.00 45.00 3.00 48.00 10.00 \n",
            "8.00 12.00 72.00 0.00 42.00 50.00 9.00 60.00 35.00 0.00 \n",
            "10.00 0.00 0.00 0.00 0.00 16.00 30.00 49.00 0.00 40.00 \n",
            "40.00 18.00 50.00 35.00 90.00 100.00 12.00 16.00 15.00 0.00 \n",
            "21.00 18.00 28.00 9.00 8.00 8.00 15.00 0.00 36.00 4.00 \n",
            "24.00 3.00 30.00 6.00 2.00 4.00 90.00 32.00 0.00 4.00 \n",
            "==2850== Profiling application: ./CUDA_hadamard100\n",
            "==2850== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  6.4069ms        30  213.56us  211.93us  214.17us  cuda_hadamard(unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   93.87%  214.51ms         3  71.502ms  50.550us  214.40ms  cudaMallocManaged\n",
            "                    2.86%  6.5383ms        30  217.94us  211.20us  219.18us  cudaEventSynchronize\n",
            "                    1.26%  2.8795ms         2  1.4398ms  5.8730us  2.8737ms  cudaDeviceSynchronize\n",
            "                    0.93%  2.1291ms         3  709.69us  690.74us  734.01us  cudaFree\n",
            "                    0.77%  1.7559ms         3  585.31us  17.552us  1.5669ms  cudaMemPrefetchAsync\n",
            "                    0.15%  333.02us        30  11.100us  3.7230us  193.75us  cudaLaunchKernel\n",
            "                    0.06%  147.54us       114  1.2940us     102ns  58.269us  cuDeviceGetAttribute\n",
            "                    0.05%  115.16us        60  1.9190us  1.2390us  12.739us  cudaEventRecord\n",
            "                    0.01%  31.276us        30  1.0420us     923ns  1.9520us  cudaEventElapsedTime\n",
            "                    0.01%  22.767us         5  4.5530us  1.2480us  15.935us  cudaMemAdvise\n",
            "                    0.01%  14.203us         2  7.1010us  1.0260us  13.177us  cudaEventCreate\n",
            "                    0.01%  12.776us         1  12.776us  12.776us  12.776us  cuDeviceGetName\n",
            "                    0.00%  5.3940us         1  5.3940us  5.3940us  5.3940us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.0800us        30     136ns      98ns     662ns  cudaGetLastError\n",
            "                    0.00%  3.0540us         2  1.5270us     367ns  2.6870us  cudaEventDestroy\n",
            "                    0.00%  2.0960us         1  2.0960us  2.0960us  2.0960us  cudaGetDevice\n",
            "                    0.00%  2.0280us         3     676ns     106ns  1.7290us  cuDeviceGetCount\n",
            "                    0.00%     926ns         2     463ns     133ns     793ns  cuDeviceGet\n",
            "                    0.00%     603ns         1     603ns     603ns     603ns  cuModuleGetLoadingMode\n",
            "                    0.00%     414ns         1     414ns     414ns     414ns  cuDeviceTotalMem\n",
            "                    0.00%     249ns         1     249ns     249ns     249ns  cuDeviceGetUuid\n",
            "\n",
            "==2850== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      16  2.0000MB  2.0000MB  2.0000MB  32.00000MB  2.771545ms  Host To Device\n",
            "       4  32.000KB  4.0000KB  60.000KB  128.0000KB  18.27100us  Device To Host\n",
            "Total CPU Page faults: 8194\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 2048x2048 THREADSIZE: 32x32\n",
        "WRITE FILE = hadamard101"
      ],
      "metadata": {
        "id": "R2NrdLkUt338"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard101.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10  // Ensures values are between 0 and 10\n",
        "#define ARRAY_SIZE 2048 // Updated matrix size\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name);\n",
        "\n",
        "__global__\n",
        "void cuda_hadamard(size_t numCol, size_t numRow, float* Z, float* X, float* Y) {\n",
        "    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < numRow && col < numCol) {\n",
        "        size_t idx = row * numCol + col;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = ARRAY_SIZE * ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    // Unified Memory Allocation\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        for (size_t j = 0; j < ARRAY_SIZE; ++j) {\n",
        "            size_t idx = i * ARRAY_SIZE + j;\n",
        "            X[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "            Y[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Configure kernel launch parameters\n",
        "    dim3 threadsPerBlock(32, 32);\n",
        "    dim3 numBlocks((ARRAY_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ARRAY_SIZE + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Timing setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard<<<numBlocks, threadsPerBlock>>>(ARRAY_SIZE, ARRAY_SIZE, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before printing results\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Print initial matrices (only first 10x10 elements)\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, X, \"X\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Y, \"Y\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Z, \"Z\");\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// Corrected function to print only the first 10x10 elements\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name) {\n",
        "    printf(\"\\nMatrix %s (First 10x10 elements):\\n\", name);\n",
        "    for (size_t i = 0; i < 10; ++i) {  // Print first 10 rows\n",
        "        for (size_t j = 0; j < 10; ++j) {  // Print first 10 columns\n",
        "            printf(\"%.2f \", Arr[i * numCol + j]);  // Corrected indexing\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0HsZbnVuAEZ",
        "outputId": "cc8b9f0c-15a4-4c8d-fbc7-5945486c9e8a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CUDA_hadamard101.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard101 CUDA_hadamard101.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard101"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCTFn8uluFgP",
        "outputId": "f8ced91f-62ac-4103-c83a-57674030407a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==3001== NVPROF is profiling process 3001, command: ./CUDA_hadamard101\n",
            "Average execution time over 30 runs: 0.2497 ms\n",
            "\n",
            "Matrix X (First 10x10 elements):\n",
            "10.00 9.00 9.00 4.00 2.00 7.00 4.00 4.00 9.00 7.00 \n",
            "6.00 2.00 2.00 10.00 8.00 3.00 3.00 4.00 0.00 6.00 \n",
            "1.00 0.00 8.00 10.00 2.00 5.00 0.00 4.00 5.00 7.00 \n",
            "1.00 0.00 1.00 9.00 6.00 6.00 0.00 8.00 4.00 8.00 \n",
            "2.00 2.00 2.00 5.00 2.00 0.00 9.00 9.00 2.00 10.00 \n",
            "7.00 3.00 3.00 4.00 1.00 8.00 2.00 6.00 3.00 3.00 \n",
            "0.00 2.00 3.00 5.00 2.00 10.00 4.00 10.00 0.00 7.00 \n",
            "10.00 0.00 1.00 6.00 3.00 4.00 5.00 8.00 7.00 1.00 \n",
            "6.00 1.00 6.00 10.00 5.00 9.00 6.00 6.00 0.00 4.00 \n",
            "10.00 8.00 2.00 3.00 8.00 4.00 8.00 3.00 10.00 7.00 \n",
            "\n",
            "Matrix Y (First 10x10 elements):\n",
            "8.00 10.00 0.00 1.00 6.00 3.00 7.00 6.00 10.00 3.00 \n",
            "4.00 9.00 0.00 5.00 0.00 10.00 10.00 3.00 3.00 10.00 \n",
            "6.00 6.00 5.00 2.00 2.00 3.00 9.00 7.00 3.00 1.00 \n",
            "1.00 3.00 2.00 10.00 1.00 4.00 9.00 10.00 6.00 3.00 \n",
            "10.00 9.00 1.00 8.00 10.00 1.00 10.00 6.00 10.00 1.00 \n",
            "7.00 0.00 3.00 1.00 8.00 10.00 9.00 7.00 9.00 5.00 \n",
            "9.00 1.00 3.00 8.00 7.00 0.00 10.00 5.00 9.00 2.00 \n",
            "1.00 5.00 3.00 10.00 5.00 10.00 6.00 6.00 6.00 3.00 \n",
            "8.00 3.00 10.00 7.00 0.00 7.00 3.00 2.00 2.00 0.00 \n",
            "9.00 2.00 8.00 6.00 7.00 6.00 3.00 3.00 0.00 4.00 \n",
            "\n",
            "Matrix Z (First 10x10 elements):\n",
            "80.00 90.00 0.00 4.00 12.00 21.00 28.00 24.00 90.00 21.00 \n",
            "24.00 18.00 0.00 50.00 0.00 30.00 30.00 12.00 0.00 60.00 \n",
            "6.00 0.00 40.00 20.00 4.00 15.00 0.00 28.00 15.00 7.00 \n",
            "1.00 0.00 2.00 90.00 6.00 24.00 0.00 80.00 24.00 24.00 \n",
            "20.00 18.00 2.00 40.00 20.00 0.00 90.00 54.00 20.00 10.00 \n",
            "49.00 0.00 9.00 4.00 8.00 80.00 18.00 42.00 27.00 15.00 \n",
            "0.00 2.00 9.00 40.00 14.00 0.00 40.00 50.00 0.00 14.00 \n",
            "10.00 0.00 3.00 60.00 15.00 40.00 30.00 48.00 42.00 3.00 \n",
            "48.00 3.00 60.00 70.00 0.00 63.00 18.00 12.00 0.00 0.00 \n",
            "90.00 16.00 16.00 18.00 56.00 24.00 24.00 9.00 0.00 28.00 \n",
            "==3001== Profiling application: ./CUDA_hadamard101\n",
            "==3001== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  7.1316ms        30  237.72us  236.67us  238.72us  cuda_hadamard(unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   93.73%  205.07ms         3  68.358ms  32.114us  205.00ms  cudaMallocManaged\n",
            "                    3.32%  7.2650ms        30  242.17us  233.50us  243.71us  cudaEventSynchronize\n",
            "                    1.66%  3.6352ms         3  1.2117ms  80.653us  1.9381ms  cudaMemPrefetchAsync\n",
            "                    0.97%  2.1331ms         3  711.02us  703.32us  718.17us  cudaFree\n",
            "                    0.15%  326.22us        30  10.873us  3.6750us  165.46us  cudaLaunchKernel\n",
            "                    0.06%  137.09us       114  1.2020us     103ns  57.253us  cuDeviceGetAttribute\n",
            "                    0.06%  122.43us        60  2.0400us  1.2570us  11.924us  cudaEventRecord\n",
            "                    0.02%  34.240us        30  1.1410us     938ns  2.2190us  cudaEventElapsedTime\n",
            "                    0.01%  20.648us         5  4.1290us  1.3570us  13.512us  cudaMemAdvise\n",
            "                    0.01%  14.363us         2  7.1810us  6.4040us  7.9590us  cudaDeviceSynchronize\n",
            "                    0.01%  12.190us         2  6.0950us     807ns  11.383us  cudaEventCreate\n",
            "                    0.00%  10.509us         1  10.509us  10.509us  10.509us  cuDeviceGetName\n",
            "                    0.00%  5.0280us         1  5.0280us  5.0280us  5.0280us  cuDeviceGetPCIBusId\n",
            "                    0.00%  3.8020us        30     126ns      88ns     287ns  cudaGetLastError\n",
            "                    0.00%  2.7360us         2  1.3680us     439ns  2.2970us  cudaEventDestroy\n",
            "                    0.00%  2.2220us         1  2.2220us  2.2220us  2.2220us  cudaGetDevice\n",
            "                    0.00%  1.4610us         3     487ns     150ns  1.0970us  cuDeviceGetCount\n",
            "                    0.00%     712ns         2     356ns     113ns     599ns  cuDeviceGet\n",
            "                    0.00%     487ns         1     487ns     487ns     487ns  cuModuleGetLoadingMode\n",
            "                    0.00%     422ns         1     422ns     422ns     422ns  cuDeviceTotalMem\n",
            "                    0.00%     227ns         1     227ns     227ns     227ns  cuDeviceGetUuid\n",
            "\n",
            "==3001== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      16  2.0000MB  2.0000MB  2.0000MB  32.00000MB  2.769691ms  Host To Device\n",
            "       4  32.000KB  4.0000KB  60.000KB  128.0000KB  18.56000us  Device To Host\n",
            "Total CPU Page faults: 8194\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4096x4096"
      ],
      "metadata": {
        "id": "ePUHc4HsuL8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 4096x4096 THREADSIZE: 8x8\n",
        "WRITE FILE = hadamard110"
      ],
      "metadata": {
        "id": "veEQygpnuPVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard110.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10  // Ensures values are between 0 and 10\n",
        "#define ARRAY_SIZE 4096 // Updated matrix size\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name);\n",
        "\n",
        "__global__\n",
        "void cuda_hadamard(size_t numCol, size_t numRow, float* Z, float* X, float* Y) {\n",
        "    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < numRow && col < numCol) {\n",
        "        size_t idx = row * numCol + col;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = ARRAY_SIZE * ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    // Unified Memory Allocation\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        for (size_t j = 0; j < ARRAY_SIZE; ++j) {\n",
        "            size_t idx = i * ARRAY_SIZE + j;\n",
        "            X[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "            Y[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Configure kernel launch parameters\n",
        "    dim3 threadsPerBlock(8, 8);\n",
        "    dim3 numBlocks((ARRAY_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ARRAY_SIZE + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Timing setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard<<<numBlocks, threadsPerBlock>>>(ARRAY_SIZE, ARRAY_SIZE, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before printing results\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Print initial matrices (only first 10x10 elements)\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, X, \"X\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Y, \"Y\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Z, \"Z\");\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// Corrected function to print only the first 10x10 elements\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name) {\n",
        "    printf(\"\\nMatrix %s (First 10x10 elements):\\n\", name);\n",
        "    for (size_t i = 0; i < 10; ++i) {  // Print first 10 rows\n",
        "        for (size_t j = 0; j < 10; ++j) {  // Print first 10 columns\n",
        "            printf(\"%.2f \", Arr[i * numCol + j]);  // Corrected indexing\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtkGdqzLuNqS",
        "outputId": "3dd5139c-e772-4f6e-ad65-2b7b71332478"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CUDA_hadamard110.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard110 CUDA_hadamard110.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard110"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNH1OHtjuRiM",
        "outputId": "c5828aee-672e-4493-ed5f-0206ae711c0b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==3263== NVPROF is profiling process 3263, command: ./CUDA_hadamard110\n",
            "Average execution time over 30 runs: 0.8829 ms\n",
            "\n",
            "Matrix X (First 10x10 elements):\n",
            "1.00 10.00 1.00 2.00 10.00 5.00 0.00 2.00 1.00 7.00 \n",
            "2.00 3.00 4.00 2.00 1.00 1.00 3.00 9.00 7.00 0.00 \n",
            "5.00 4.00 6.00 1.00 7.00 1.00 8.00 6.00 5.00 6.00 \n",
            "3.00 2.00 10.00 0.00 1.00 2.00 1.00 1.00 6.00 4.00 \n",
            "8.00 9.00 7.00 1.00 0.00 6.00 0.00 4.00 1.00 5.00 \n",
            "10.00 2.00 7.00 4.00 10.00 9.00 4.00 3.00 9.00 3.00 \n",
            "6.00 3.00 7.00 4.00 0.00 7.00 7.00 1.00 10.00 8.00 \n",
            "3.00 7.00 4.00 0.00 4.00 1.00 5.00 4.00 2.00 5.00 \n",
            "1.00 0.00 6.00 8.00 8.00 0.00 0.00 6.00 4.00 10.00 \n",
            "1.00 8.00 8.00 0.00 4.00 1.00 0.00 1.00 10.00 2.00 \n",
            "\n",
            "Matrix Y (First 10x10 elements):\n",
            "6.00 10.00 4.00 9.00 10.00 3.00 5.00 5.00 5.00 7.00 \n",
            "4.00 4.00 0.00 4.00 6.00 4.00 9.00 9.00 2.00 7.00 \n",
            "7.00 3.00 8.00 1.00 8.00 8.00 5.00 0.00 1.00 4.00 \n",
            "4.00 9.00 3.00 9.00 1.00 8.00 10.00 9.00 1.00 10.00 \n",
            "2.00 2.00 3.00 9.00 6.00 0.00 3.00 8.00 0.00 9.00 \n",
            "10.00 4.00 8.00 9.00 6.00 6.00 10.00 3.00 8.00 2.00 \n",
            "4.00 1.00 3.00 9.00 7.00 1.00 6.00 9.00 8.00 8.00 \n",
            "0.00 10.00 7.00 8.00 4.00 9.00 8.00 0.00 8.00 4.00 \n",
            "9.00 10.00 4.00 9.00 9.00 8.00 4.00 6.00 1.00 0.00 \n",
            "1.00 2.00 8.00 3.00 10.00 6.00 2.00 10.00 2.00 7.00 \n",
            "\n",
            "Matrix Z (First 10x10 elements):\n",
            "6.00 100.00 4.00 18.00 100.00 15.00 0.00 10.00 5.00 49.00 \n",
            "8.00 12.00 0.00 8.00 6.00 4.00 27.00 81.00 14.00 0.00 \n",
            "35.00 12.00 48.00 1.00 56.00 8.00 40.00 0.00 5.00 24.00 \n",
            "12.00 18.00 30.00 0.00 1.00 16.00 10.00 9.00 6.00 40.00 \n",
            "16.00 18.00 21.00 9.00 0.00 0.00 0.00 32.00 0.00 45.00 \n",
            "100.00 8.00 56.00 36.00 60.00 54.00 40.00 9.00 72.00 6.00 \n",
            "24.00 3.00 21.00 36.00 0.00 7.00 42.00 9.00 80.00 64.00 \n",
            "0.00 70.00 28.00 0.00 16.00 9.00 40.00 0.00 16.00 20.00 \n",
            "9.00 0.00 24.00 72.00 72.00 0.00 0.00 36.00 4.00 0.00 \n",
            "1.00 16.00 64.00 0.00 40.00 6.00 0.00 10.00 20.00 14.00 \n",
            "==3263== Profiling application: ./CUDA_hadamard110\n",
            "==3263== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  25.947ms        30  864.89us  863.38us  866.61us  cuda_hadamard(unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   76.93%  210.54ms         3  70.181ms  28.174us  210.46ms  cudaMallocManaged\n",
            "                    9.62%  26.340ms        30  877.99us  861.75us  1.1519ms  cudaEventSynchronize\n",
            "                    8.82%  24.136ms         3  8.0455ms  405.78us  12.614ms  cudaMemPrefetchAsync\n",
            "                    4.23%  11.588ms         3  3.8626ms  1.1302ms  5.8465ms  cudaFree\n",
            "                    0.18%  503.26us        30  16.775us  6.7700us  198.03us  cudaLaunchKernel\n",
            "                    0.08%  210.74us        60  3.5120us  2.3490us  13.634us  cudaEventRecord\n",
            "                    0.05%  145.37us       114  1.2750us     102ns  62.802us  cuDeviceGetAttribute\n",
            "                    0.02%  66.411us        30  2.2130us  1.6230us  5.4290us  cudaEventElapsedTime\n",
            "                    0.01%  39.797us         2  19.898us  1.1130us  38.684us  cudaEventCreate\n",
            "                    0.01%  37.331us         1  37.331us  37.331us  37.331us  cudaGetDevice\n",
            "                    0.01%  27.428us         5  5.4850us  1.2670us  20.243us  cudaMemAdvise\n",
            "                    0.01%  15.562us         2  7.7810us  6.2170us  9.3450us  cudaDeviceSynchronize\n",
            "                    0.00%  11.405us         1  11.405us  11.405us  11.405us  cuDeviceGetName\n",
            "                    0.00%  7.0020us        30     233ns     178ns     538ns  cudaGetLastError\n",
            "                    0.00%  5.0070us         1  5.0070us  5.0070us  5.0070us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.3230us         2  2.1610us     990ns  3.3330us  cudaEventDestroy\n",
            "                    0.00%  1.4430us         3     481ns     132ns  1.1080us  cuDeviceGetCount\n",
            "                    0.00%     990ns         2     495ns     135ns     855ns  cuDeviceGet\n",
            "                    0.00%     501ns         1     501ns     501ns     501ns  cuModuleGetLoadingMode\n",
            "                    0.00%     331ns         1     331ns     331ns     331ns  cuDeviceTotalMem\n",
            "                    0.00%     265ns         1     265ns     265ns     265ns  cuDeviceGetUuid\n",
            "\n",
            "==3263== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      64  2.0000MB  2.0000MB  2.0000MB  128.0000MB  11.10868ms  Host To Device\n",
            "       6  42.666KB  4.0000KB  124.00KB  256.0000KB  31.23000us  Device To Host\n",
            "Total CPU Page faults: 32771\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 4096x4096 THREADSIZE: 16x16\n",
        "WRITE FILE = hadamard111"
      ],
      "metadata": {
        "id": "03Bdfm8zurbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard111.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10  // Ensures values are between 0 and 10\n",
        "#define ARRAY_SIZE 4096 // Updated matrix size\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name);\n",
        "\n",
        "__global__\n",
        "void cuda_hadamard(size_t numCol, size_t numRow, float* Z, float* X, float* Y) {\n",
        "    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < numRow && col < numCol) {\n",
        "        size_t idx = row * numCol + col;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = ARRAY_SIZE * ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    // Unified Memory Allocation\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        for (size_t j = 0; j < ARRAY_SIZE; ++j) {\n",
        "            size_t idx = i * ARRAY_SIZE + j;\n",
        "            X[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "            Y[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Configure kernel launch parameters\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 numBlocks((ARRAY_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ARRAY_SIZE + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Timing setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard<<<numBlocks, threadsPerBlock>>>(ARRAY_SIZE, ARRAY_SIZE, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before printing results\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Print initial matrices (only first 10x10 elements)\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, X, \"X\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Y, \"Y\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Z, \"Z\");\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// Corrected function to print only the first 10x10 elements\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name) {\n",
        "    printf(\"\\nMatrix %s (First 10x10 elements):\\n\", name);\n",
        "    for (size_t i = 0; i < 10; ++i) {  // Print first 10 rows\n",
        "        for (size_t j = 0; j < 10; ++j) {  // Print first 10 columns\n",
        "            printf(\"%.2f \", Arr[i * numCol + j]);  // Corrected indexing\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldvV7RfCurJX",
        "outputId": "1cced47b-c0e4-4093-b219-b35025069fd5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CUDA_hadamard111.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard111 CUDA_hadamard111.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard111"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1dn-Zb8uvq2",
        "outputId": "ce2a4c14-c646-48f0-ecf7-a648c3c46622"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==3525== NVPROF is profiling process 3525, command: ./CUDA_hadamard111\n",
            "Average execution time over 30 runs: 0.8648 ms\n",
            "\n",
            "Matrix X (First 10x10 elements):\n",
            "9.00 10.00 4.00 4.00 1.00 10.00 1.00 9.00 10.00 8.00 \n",
            "8.00 9.00 1.00 4.00 0.00 7.00 8.00 5.00 0.00 2.00 \n",
            "9.00 8.00 8.00 3.00 10.00 3.00 5.00 0.00 9.00 5.00 \n",
            "0.00 10.00 2.00 2.00 10.00 8.00 3.00 3.00 4.00 10.00 \n",
            "4.00 6.00 5.00 2.00 10.00 0.00 10.00 6.00 9.00 0.00 \n",
            "2.00 5.00 3.00 6.00 6.00 1.00 10.00 9.00 7.00 3.00 \n",
            "9.00 5.00 5.00 0.00 2.00 0.00 2.00 7.00 5.00 10.00 \n",
            "3.00 3.00 7.00 3.00 3.00 0.00 7.00 5.00 4.00 3.00 \n",
            "2.00 2.00 7.00 10.00 1.00 8.00 2.00 7.00 1.00 7.00 \n",
            "7.00 3.00 6.00 5.00 8.00 6.00 6.00 0.00 2.00 1.00 \n",
            "\n",
            "Matrix Y (First 10x10 elements):\n",
            "6.00 2.00 8.00 9.00 7.00 9.00 5.00 0.00 3.00 3.00 \n",
            "1.00 3.00 7.00 7.00 5.00 3.00 1.00 9.00 0.00 0.00 \n",
            "6.00 6.00 3.00 10.00 9.00 2.00 9.00 6.00 0.00 2.00 \n",
            "4.00 8.00 4.00 6.00 9.00 1.00 2.00 0.00 7.00 2.00 \n",
            "10.00 2.00 2.00 9.00 7.00 8.00 9.00 8.00 1.00 2.00 \n",
            "1.00 10.00 2.00 1.00 7.00 4.00 10.00 9.00 1.00 2.00 \n",
            "0.00 6.00 8.00 3.00 8.00 9.00 8.00 8.00 7.00 7.00 \n",
            "4.00 2.00 0.00 0.00 8.00 10.00 1.00 3.00 3.00 3.00 \n",
            "9.00 4.00 0.00 9.00 2.00 9.00 7.00 8.00 0.00 2.00 \n",
            "1.00 0.00 1.00 6.00 5.00 8.00 3.00 3.00 7.00 3.00 \n",
            "\n",
            "Matrix Z (First 10x10 elements):\n",
            "54.00 20.00 32.00 36.00 7.00 90.00 5.00 0.00 30.00 24.00 \n",
            "8.00 27.00 7.00 28.00 0.00 21.00 8.00 45.00 0.00 0.00 \n",
            "54.00 48.00 24.00 30.00 90.00 6.00 45.00 0.00 0.00 10.00 \n",
            "0.00 80.00 8.00 12.00 90.00 8.00 6.00 0.00 28.00 20.00 \n",
            "40.00 12.00 10.00 18.00 70.00 0.00 90.00 48.00 9.00 0.00 \n",
            "2.00 50.00 6.00 6.00 42.00 4.00 100.00 81.00 7.00 6.00 \n",
            "0.00 30.00 40.00 0.00 16.00 0.00 16.00 56.00 35.00 70.00 \n",
            "12.00 6.00 0.00 0.00 24.00 0.00 7.00 15.00 12.00 9.00 \n",
            "18.00 8.00 0.00 90.00 2.00 72.00 14.00 56.00 0.00 14.00 \n",
            "7.00 0.00 6.00 30.00 40.00 48.00 18.00 0.00 14.00 3.00 \n",
            "==3525== Profiling application: ./CUDA_hadamard111\n",
            "==3525== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  25.540ms        30  851.32us  849.49us  852.15us  cuda_hadamard(unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   82.46%  209.72ms         3  69.905ms  31.384us  209.62ms  cudaMallocManaged\n",
            "                   10.09%  25.669ms        30  855.62us  837.34us  865.16us  cudaEventSynchronize\n",
            "                    4.74%  12.063ms         3  4.0210ms  276.85us  5.9553ms  cudaMemPrefetchAsync\n",
            "                    2.39%  6.0863ms         3  2.0288ms  715.79us  2.8084ms  cudaFree\n",
            "                    0.15%  370.09us        30  12.336us  4.5160us  167.10us  cudaLaunchKernel\n",
            "                    0.06%  146.10us        60  2.4350us  1.3660us  12.070us  cudaEventRecord\n",
            "                    0.06%  140.62us       114  1.2330us     110ns  57.389us  cuDeviceGetAttribute\n",
            "                    0.02%  40.100us        30  1.3360us  1.0050us  3.8400us  cudaEventElapsedTime\n",
            "                    0.01%  22.794us         5  4.5580us  1.2070us  16.054us  cudaMemAdvise\n",
            "                    0.01%  14.538us         2  7.2690us  7.2210us  7.3170us  cudaDeviceSynchronize\n",
            "                    0.01%  12.787us         2  6.3930us     838ns  11.949us  cudaEventCreate\n",
            "                    0.00%  12.280us         1  12.280us  12.280us  12.280us  cuDeviceGetName\n",
            "                    0.00%  9.4720us         1  9.4720us  9.4720us  9.4720us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.6110us        30     153ns      98ns     392ns  cudaGetLastError\n",
            "                    0.00%  4.2240us         2  2.1120us     659ns  3.5650us  cudaEventDestroy\n",
            "                    0.00%  2.2100us         1  2.2100us  2.2100us  2.2100us  cudaGetDevice\n",
            "                    0.00%  1.8070us         3     602ns     166ns  1.1390us  cuDeviceGetCount\n",
            "                    0.00%     975ns         1     975ns     975ns     975ns  cuModuleGetLoadingMode\n",
            "                    0.00%     744ns         2     372ns     124ns     620ns  cuDeviceGet\n",
            "                    0.00%     413ns         1     413ns     413ns     413ns  cuDeviceTotalMem\n",
            "                    0.00%     294ns         1     294ns     294ns     294ns  cuDeviceGetUuid\n",
            "\n",
            "==3525== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      64  2.0000MB  2.0000MB  2.0000MB  128.0000MB  11.10910ms  Host To Device\n",
            "       6  42.666KB  4.0000KB  124.00KB  256.0000KB  31.42400us  Device To Host\n",
            "Total CPU Page faults: 32771\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 4096x4096 THREADSIZE: 32x32\n",
        "WRITE FILE = hadamard1000"
      ],
      "metadata": {
        "id": "uZS8mmsyvLsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard1000.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10  // Ensures values are between 0 and 10\n",
        "#define ARRAY_SIZE 4096 // Updated matrix size\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name);\n",
        "\n",
        "__global__\n",
        "void cuda_hadamard(size_t numCol, size_t numRow, float* Z, float* X, float* Y) {\n",
        "    size_t row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < numRow && col < numCol) {\n",
        "        size_t idx = row * numCol + col;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = ARRAY_SIZE * ARRAY_SIZE * sizeof(float);\n",
        "\n",
        "    // Unified Memory Allocation\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize matrices with random values\n",
        "    for (size_t i = 0; i < ARRAY_SIZE; ++i) {\n",
        "        for (size_t j = 0; j < ARRAY_SIZE; ++j) {\n",
        "            size_t idx = i * ARRAY_SIZE + j;\n",
        "            X[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "            Y[idx] = (float)(rand() % (RANDOM_MAX + 1));  // Random 0-10\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Configure kernel launch parameters\n",
        "    dim3 threadsPerBlock(32, 32);\n",
        "    dim3 numBlocks((ARRAY_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ARRAY_SIZE + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Timing setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard<<<numBlocks, threadsPerBlock>>>(ARRAY_SIZE, ARRAY_SIZE, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before printing results\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Print initial matrices (only first 10x10 elements)\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, X, \"X\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Y, \"Y\");\n",
        "    print_matrix(ARRAY_SIZE, ARRAY_SIZE, Z, \"Z\");\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "// Corrected function to print only the first 10x10 elements\n",
        "void print_matrix(size_t numCol, size_t numRow, float* Arr, const char* name) {\n",
        "    printf(\"\\nMatrix %s (First 10x10 elements):\\n\", name);\n",
        "    for (size_t i = 0; i < 10; ++i) {  // Print first 10 rows\n",
        "        for (size_t j = 0; j < 10; ++j) {  // Print first 10 columns\n",
        "            printf(\"%.2f \", Arr[i * numCol + j]);  // Corrected indexing\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7Uy45a5wAf1",
        "outputId": "11d270c7-4186-4cf7-d751-a260eb7ded80"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CUDA_hadamard1000.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard1000 CUDA_hadamard100.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard1000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukI-K7B7wAe7",
        "outputId": "b6310833-f4f9-4e45-9fe8-2520dae7fd94"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==3689== NVPROF is profiling process 3689, command: ./CUDA_hadamard1000\n",
            "Average execution time over 30 runs: 0.2265 ms\n",
            "\n",
            "Matrix X (First 10x10 elements):\n",
            "6.00 10.00 0.00 2.00 7.00 10.00 2.00 1.00 10.00 9.00 \n",
            "3.00 10.00 4.00 3.00 3.00 10.00 9.00 8.00 3.00 2.00 \n",
            "4.00 9.00 0.00 7.00 6.00 6.00 8.00 10.00 2.00 4.00 \n",
            "6.00 1.00 8.00 3.00 5.00 6.00 4.00 9.00 2.00 4.00 \n",
            "7.00 9.00 7.00 2.00 2.00 8.00 10.00 3.00 8.00 2.00 \n",
            "2.00 6.00 4.00 8.00 5.00 3.00 0.00 8.00 2.00 3.00 \n",
            "6.00 10.00 10.00 10.00 1.00 2.00 1.00 9.00 3.00 7.00 \n",
            "8.00 4.00 3.00 4.00 7.00 5.00 5.00 1.00 4.00 6.00 \n",
            "8.00 4.00 6.00 3.00 7.00 8.00 5.00 10.00 8.00 7.00 \n",
            "2.00 4.00 3.00 7.00 1.00 10.00 0.00 2.00 9.00 0.00 \n",
            "\n",
            "Matrix Y (First 10x10 elements):\n",
            "2.00 0.00 5.00 1.00 8.00 8.00 2.00 7.00 0.00 8.00 \n",
            "9.00 5.00 4.00 9.00 9.00 2.00 10.00 1.00 2.00 6.00 \n",
            "9.00 3.00 3.00 4.00 5.00 7.00 1.00 2.00 2.00 8.00 \n",
            "10.00 0.00 0.00 8.00 10.00 0.00 1.00 0.00 1.00 5.00 \n",
            "0.00 1.00 0.00 6.00 5.00 0.00 6.00 10.00 8.00 1.00 \n",
            "0.00 5.00 1.00 6.00 7.00 9.00 0.00 1.00 1.00 2.00 \n",
            "3.00 2.00 2.00 8.00 6.00 3.00 2.00 3.00 8.00 0.00 \n",
            "4.00 10.00 9.00 5.00 0.00 0.00 1.00 6.00 3.00 8.00 \n",
            "2.00 2.00 4.00 0.00 1.00 2.00 4.00 6.00 4.00 6.00 \n",
            "8.00 7.00 4.00 6.00 2.00 0.00 2.00 0.00 4.00 9.00 \n",
            "\n",
            "Matrix Z (First 10x10 elements):\n",
            "12.00 0.00 0.00 2.00 56.00 80.00 4.00 7.00 0.00 72.00 \n",
            "27.00 50.00 16.00 27.00 27.00 20.00 90.00 8.00 6.00 12.00 \n",
            "36.00 27.00 0.00 28.00 30.00 42.00 8.00 20.00 4.00 32.00 \n",
            "60.00 0.00 0.00 24.00 50.00 0.00 4.00 0.00 2.00 20.00 \n",
            "0.00 9.00 0.00 12.00 10.00 0.00 60.00 30.00 64.00 2.00 \n",
            "0.00 30.00 4.00 48.00 35.00 27.00 0.00 8.00 2.00 6.00 \n",
            "18.00 20.00 20.00 80.00 6.00 6.00 2.00 27.00 24.00 0.00 \n",
            "32.00 40.00 27.00 20.00 0.00 0.00 5.00 6.00 12.00 48.00 \n",
            "16.00 8.00 24.00 0.00 7.00 16.00 20.00 60.00 32.00 42.00 \n",
            "16.00 28.00 12.00 42.00 2.00 0.00 0.00 0.00 36.00 0.00 \n",
            "==3689== Profiling application: ./CUDA_hadamard1000\n",
            "==3689== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  6.4055ms        30  213.52us  211.55us  214.05us  cuda_hadamard(unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   93.63%  211.53ms         3  70.510ms  38.638us  211.44ms  cudaMallocManaged\n",
            "                    2.89%  6.5380ms        30  217.93us  202.17us  231.09us  cudaEventSynchronize\n",
            "                    1.30%  2.9430ms         2  1.4715ms  7.2690us  2.9358ms  cudaDeviceSynchronize\n",
            "                    1.02%  2.2961ms         3  765.36us  702.67us  862.00us  cudaFree\n",
            "                    0.81%  1.8404ms         3  613.47us  19.773us  1.6546ms  cudaMemPrefetchAsync\n",
            "                    0.16%  353.07us        30  11.769us  3.7210us  162.86us  cudaLaunchKernel\n",
            "                    0.07%  151.48us       114  1.3280us     114ns  63.144us  cuDeviceGetAttribute\n",
            "                    0.06%  146.03us        60  2.4330us  1.2810us  10.159us  cudaEventRecord\n",
            "                    0.03%  56.932us        30  1.8970us     922ns  18.584us  cudaEventElapsedTime\n",
            "                    0.01%  19.850us         5  3.9700us  1.2290us  12.984us  cudaMemAdvise\n",
            "                    0.01%  17.044us         2  8.5220us     751ns  16.293us  cudaEventCreate\n",
            "                    0.01%  11.791us         1  11.791us  11.791us  11.791us  cuDeviceGetName\n",
            "                    0.00%  5.1010us        30     170ns      86ns     837ns  cudaGetLastError\n",
            "                    0.00%  4.8980us         1  4.8980us  4.8980us  4.8980us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.7190us         2  2.3590us     778ns  3.9410us  cudaEventDestroy\n",
            "                    0.00%  1.9000us         3     633ns     139ns  1.5680us  cuDeviceGetCount\n",
            "                    0.00%  1.8870us         1  1.8870us  1.8870us  1.8870us  cudaGetDevice\n",
            "                    0.00%     678ns         2     339ns     134ns     544ns  cuDeviceGet\n",
            "                    0.00%     466ns         1     466ns     466ns     466ns  cuModuleGetLoadingMode\n",
            "                    0.00%     426ns         1     426ns     426ns     426ns  cuDeviceTotalMem\n",
            "                    0.00%     282ns         1     282ns     282ns     282ns  cuDeviceGetUuid\n",
            "\n",
            "==3689== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "      16  2.0000MB  2.0000MB  2.0000MB  32.00000MB  2.778267ms  Host To Device\n",
            "       4  32.000KB  4.0000KB  60.000KB  128.0000KB  17.02400us  Device To Host\n",
            "Total CPU Page faults: 8194\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Dive: Hadamard product using 3D variables"
      ],
      "metadata": {
        "id": "y9fUDd6RxCYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WRITEFILE CODES\n",
        "\n",
        "3D000 - 1024x1024 - 8x8\n",
        "\n",
        "3D001 - 1024x1024 - 16x16\n",
        "\n"
      ],
      "metadata": {
        "id": "1w8Lt5MmxEXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1024x1024"
      ],
      "metadata": {
        "id": "HNM4N_VzxQe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 1024x1024 THREADSIZE: 8x8x8\n",
        "WRITE FILE = hadamard3D000"
      ],
      "metadata": {
        "id": "aJZ4MK2xxSYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard3D000.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10      // Values between 0-10\n",
        "#define DEPTH 1024         // Z-dimension\n",
        "#define ROWS 1024          // Y-dimension\n",
        "#define COLS 1024          // X-dimension\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "// 3D CUDA Kernel for Hadamard Product\n",
        "__global__\n",
        "void cuda_hadamard_3D(size_t depth, size_t rows, size_t cols, float* Z, float* X, float* Y) {\n",
        "    size_t x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    size_t y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t z = blockIdx.z * blockDim.z + threadIdx.z;\n",
        "\n",
        "    if (x < cols && y < rows && z < depth) {\n",
        "        size_t idx = (z * rows * cols) + (y * cols) + x;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = DEPTH * ROWS * COLS * sizeof(float);\n",
        "\n",
        "    // Allocate Unified Memory\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice for optimization\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize Matrices with random values\n",
        "    for (size_t i = 0; i < DEPTH; ++i) {\n",
        "        for (size_t j = 0; j < ROWS; ++j) {\n",
        "            for (size_t k = 0; k < COLS; ++k) {\n",
        "                size_t idx = (i * ROWS * COLS) + (j * COLS) + k;\n",
        "                X[idx] = (float)(rand() % (RANDOM_MAX + 1));\n",
        "                Y[idx] = (float)(rand() % (RANDOM_MAX + 1));\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch Data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Define **3D** Thread and Block Dimensions\n",
        "    dim3 threadsPerBlock(8, 8, 8);  // 8x8x8 threads per block\n",
        "    dim3 numBlocks((COLS + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ROWS + threadsPerBlock.y - 1) / threadsPerBlock.y,\n",
        "                   (DEPTH + threadsPerBlock.z - 1) / threadsPerBlock.z);\n",
        "\n",
        "    // Timing Setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    // Run Kernel 30 times & Measure Performance\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard_3D<<<numBlocks, threadsPerBlock>>>(DEPTH, ROWS, COLS, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before freeing memory\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3hQvTfT2Awg",
        "outputId": "efaba236-b658-4747-c12c-5139436be0d7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing CUDA_hadamard3D000.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard3D000 CUDA_hadamard3D000.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard3D000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8P7fdhl4NSg",
        "outputId": "723bf089-3e47-45a0-e941-aa06687618a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==17269== NVPROF is profiling process 17269, command: ./CUDA_hadamard3D000\n",
            "Average execution time over 30 runs: 55.6947 ms\n",
            "==17269== Profiling application: ./CUDA_hadamard3D000\n",
            "==17269== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  1.66977s        30  55.659ms  55.460ms  56.205ms  cuda_hadamard_3D(unsigned long, unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   48.58%  1.66999s        30  55.666ms  55.465ms  56.216ms  cudaEventSynchronize\n",
            "                   35.38%  1.21612s         3  405.37ms  23.164ms  682.59ms  cudaMemPrefetchAsync\n",
            "                   10.30%  354.21ms         3  118.07ms  26.276ms  164.06ms  cudaFree\n",
            "                    5.69%  195.63ms         3  65.210ms  57.214us  195.49ms  cudaMallocManaged\n",
            "                    0.03%  992.38us        30  33.079us  15.467us  266.92us  cudaLaunchKernel\n",
            "                    0.01%  421.29us        60  7.0210us  2.3260us  23.878us  cudaEventRecord\n",
            "                    0.00%  147.91us        30  4.9300us  2.1290us  14.295us  cudaEventElapsedTime\n",
            "                    0.00%  130.05us       114  1.1400us     109ns  53.211us  cuDeviceGetAttribute\n",
            "                    0.00%  37.200us         5  7.4400us  2.6520us  24.687us  cudaMemAdvise\n",
            "                    0.00%  28.186us         2  14.093us  9.5450us  18.641us  cudaDeviceSynchronize\n",
            "                    0.00%  20.640us         2  10.320us  1.1690us  19.471us  cudaEventCreate\n",
            "                    0.00%  10.687us         1  10.687us  10.687us  10.687us  cuDeviceGetName\n",
            "                    0.00%  9.9570us        30     331ns     159ns     557ns  cudaGetLastError\n",
            "                    0.00%  9.2190us         1  9.2190us  9.2190us  9.2190us  cudaGetDevice\n",
            "                    0.00%  7.3940us         2  3.6970us     524ns  6.8700us  cudaEventDestroy\n",
            "                    0.00%  5.3250us         1  5.3250us  5.3250us  5.3250us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.4870us         3     495ns     135ns  1.1890us  cuDeviceGetCount\n",
            "                    0.00%     864ns         2     432ns     165ns     699ns  cuDeviceGet\n",
            "                    0.00%     457ns         1     457ns     457ns     457ns  cuModuleGetLoadingMode\n",
            "                    0.00%     315ns         1     315ns     315ns     315ns  cuDeviceTotalMem\n",
            "                    0.00%     213ns         1     213ns     213ns     213ns  cuDeviceGetUuid\n",
            "\n",
            "==17269== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "    4096  2.0000MB  2.0000MB  2.0000MB  8.000000GB  710.4040ms  Host To Device\n",
            "Total CPU Page faults: 2097152\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ARRAY SIZE: 1024x1024 THREADSIZE: 16x16x16\n",
        "WRITE FILE = hadamard3D000"
      ],
      "metadata": {
        "id": "zLxSe57X4z5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile CUDA_hadamard3D001.cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define RANDOM_MAX 10      // Values between 0-10\n",
        "#define DEPTH 1024         // Z-dimension\n",
        "#define ROWS 1024          // Y-dimension\n",
        "#define COLS 1024          // X-dimension\n",
        "\n",
        "#define CHECK_CUDA(call)                                                        \\\n",
        "    do {                                                                         \\\n",
        "        cudaError_t err = call;                                                  \\\n",
        "        if (err != cudaSuccess) {                                                \\\n",
        "            fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__,     \\\n",
        "                    cudaGetErrorString(err));                                    \\\n",
        "            exit(1);                                                             \\\n",
        "        }                                                                        \\\n",
        "    } while (0)\n",
        "\n",
        "// 3D CUDA Kernel for Hadamard Product\n",
        "__global__\n",
        "void cuda_hadamard_3D(size_t depth, size_t rows, size_t cols, float* Z, float* X, float* Y) {\n",
        "    size_t x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    size_t y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    size_t z = blockIdx.z * blockDim.z + threadIdx.z;\n",
        "\n",
        "    if (x < cols && y < rows && z < depth) {\n",
        "        size_t idx = (z * rows * cols) + (y * cols) + x;\n",
        "        Z[idx] = X[idx] * Y[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    srand(time(NULL));\n",
        "    size_t ARRAY_BYTES = DEPTH * ROWS * COLS * sizeof(float);\n",
        "\n",
        "    // Allocate Unified Memory\n",
        "    float *X, *Y, *Z;\n",
        "    CHECK_CUDA(cudaMallocManaged(&X, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Y, ARRAY_BYTES));\n",
        "    CHECK_CUDA(cudaMallocManaged(&Z, ARRAY_BYTES));\n",
        "\n",
        "    // Get GPU ID\n",
        "    int device = -1;\n",
        "    CHECK_CUDA(cudaGetDevice(&device));\n",
        "\n",
        "    // Memory Advice for optimization\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(X, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Y, ARRAY_BYTES, cudaMemAdviseSetReadMostly, cudaCpuDeviceId));\n",
        "    CHECK_CUDA(cudaMemAdvise(Z, ARRAY_BYTES, cudaMemAdviseSetPreferredLocation, device));\n",
        "\n",
        "    // Initialize Matrices with random values\n",
        "    for (size_t i = 0; i < DEPTH; ++i) {\n",
        "        for (size_t j = 0; j < ROWS; ++j) {\n",
        "            for (size_t k = 0; k < COLS; ++k) {\n",
        "                size_t idx = (i * ROWS * COLS) + (j * COLS) + k;\n",
        "                X[idx] = (float)(rand() % (RANDOM_MAX + 1));\n",
        "                Y[idx] = (float)(rand() % (RANDOM_MAX + 1));\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Prefetch Data to GPU\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(X, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Y, ARRAY_BYTES, device, NULL));\n",
        "    CHECK_CUDA(cudaMemPrefetchAsync(Z, ARRAY_BYTES, device, NULL));\n",
        "\n",
        "    // Ensure data is fully transferred before kernel execution\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Define **3D** Thread and Block Dimensions\n",
        "    dim3 threadsPerBlock(16, 16, 4);  //\n",
        "    dim3 numBlocks((COLS + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                   (ROWS + threadsPerBlock.y - 1) / threadsPerBlock.y,\n",
        "                   (DEPTH + threadsPerBlock.z - 1) / threadsPerBlock.z);\n",
        "\n",
        "    // Timing Setup\n",
        "    float total_time = 0.0f;\n",
        "    int runs = 30;\n",
        "    cudaEvent_t start, stop;\n",
        "    CHECK_CUDA(cudaEventCreate(&start));\n",
        "    CHECK_CUDA(cudaEventCreate(&stop));\n",
        "\n",
        "    // Run Kernel 30 times & Measure Performance\n",
        "    for (int i = 0; i < runs; ++i) {\n",
        "        CHECK_CUDA(cudaEventRecord(start));\n",
        "        cuda_hadamard_3D<<<numBlocks, threadsPerBlock>>>(DEPTH, ROWS, COLS, Z, X, Y);\n",
        "        CHECK_CUDA(cudaGetLastError());  // Catch kernel launch errors\n",
        "        CHECK_CUDA(cudaEventRecord(stop));\n",
        "        CHECK_CUDA(cudaEventSynchronize(stop));\n",
        "\n",
        "        float milliseconds = 0;\n",
        "        CHECK_CUDA(cudaEventElapsedTime(&milliseconds, start, stop));\n",
        "        total_time += milliseconds;\n",
        "    }\n",
        "\n",
        "    printf(\"Average execution time over %d runs: %.4f ms\\n\", runs, total_time / runs);\n",
        "\n",
        "    // Synchronize before freeing memory\n",
        "    CHECK_CUDA(cudaDeviceSynchronize());\n",
        "\n",
        "    // Free allocated memory\n",
        "    CHECK_CUDA(cudaFree(X));\n",
        "    CHECK_CUDA(cudaFree(Y));\n",
        "    CHECK_CUDA(cudaFree(Z));\n",
        "\n",
        "    CHECK_CUDA(cudaEventDestroy(start));\n",
        "    CHECK_CUDA(cudaEventDestroy(stop));\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShZ2uQ2040Tz",
        "outputId": "563a4ce3-7ef6-4616-ea41-dc0aab2b8b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting CUDA_hadamard3D001.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc -o CUDA_hadamard3D001 CUDA_hadamard3D001.cu -arch=sm_75\n",
        "nvprof ./CUDA_hadamard3D001"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0wvuNc95G-q",
        "outputId": "29743095-cf07-4e58-f04d-055d41e65b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==18889== NVPROF is profiling process 18889, command: ./CUDA_hadamard3D001\n",
            "Average execution time over 30 runs: 55.0947 ms\n",
            "==18889== Profiling application: ./CUDA_hadamard3D001\n",
            "==18889== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:  100.00%  1.65185s        30  55.062ms  55.005ms  55.425ms  cuda_hadamard_3D(unsigned long, unsigned long, unsigned long, float*, float*, float*)\n",
            "      API calls:   53.75%  1.65216s        30  55.072ms  55.014ms  55.436ms  cudaEventSynchronize\n",
            "                   27.56%  847.25ms         3  282.42ms  16.702ms  416.56ms  cudaMemPrefetchAsync\n",
            "                   11.55%  354.93ms         3  118.31ms  26.270ms  164.83ms  cudaFree\n",
            "                    7.09%  217.82ms         3  72.608ms  51.136us  217.67ms  cudaMallocManaged\n",
            "                    0.03%  917.87us        30  30.595us  16.256us  212.88us  cudaLaunchKernel\n",
            "                    0.01%  386.65us        60  6.4440us  2.2030us  18.790us  cudaEventRecord\n",
            "                    0.00%  136.01us       114  1.1930us     112ns  56.090us  cuDeviceGetAttribute\n",
            "                    0.00%  132.15us        30  4.4050us  2.9510us  7.7190us  cudaEventElapsedTime\n",
            "                    0.00%  32.050us         5  6.4100us  2.5800us  18.903us  cudaMemAdvise\n",
            "                    0.00%  29.794us         2  14.897us  10.286us  19.508us  cudaDeviceSynchronize\n",
            "                    0.00%  20.656us         2  10.328us     670ns  19.986us  cudaEventCreate\n",
            "                    0.00%  15.326us         1  15.326us  15.326us  15.326us  cuDeviceGetName\n",
            "                    0.00%  10.381us        30     346ns     234ns     533ns  cudaGetLastError\n",
            "                    0.00%  7.5330us         2  3.7660us     547ns  6.9860us  cudaEventDestroy\n",
            "                    0.00%  5.7790us         1  5.7790us  5.7790us  5.7790us  cuDeviceGetPCIBusId\n",
            "                    0.00%  2.3410us         1  2.3410us  2.3410us  2.3410us  cudaGetDevice\n",
            "                    0.00%  1.4250us         3     475ns     128ns     939ns  cuDeviceGetCount\n",
            "                    0.00%     809ns         2     404ns     229ns     580ns  cuDeviceGet\n",
            "                    0.00%     641ns         1     641ns     641ns     641ns  cuModuleGetLoadingMode\n",
            "                    0.00%     462ns         1     462ns     462ns     462ns  cuDeviceTotalMem\n",
            "                    0.00%     311ns         1     311ns     311ns     311ns  cuDeviceGetUuid\n",
            "\n",
            "==18889== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "    4096  2.0000MB  2.0000MB  2.0000MB  8.000000GB  710.3877ms  Host To Device\n",
            "Total CPU Page faults: 2097152\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}